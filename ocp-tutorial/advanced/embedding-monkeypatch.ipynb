{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32dc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocpmodels.models.gemnet_oc.gemnet_oc import GemNetOC\n",
    "\n",
    "import torch\n",
    "from ocpmodels.common.utils import conditional_grad, scatter_det\n",
    "    \n",
    "    \n",
    "@conditional_grad(torch.enable_grad())\n",
    "def newforward(self, data):\n",
    "    pos = data.pos\n",
    "    batch = data.batch\n",
    "    atomic_numbers = data.atomic_numbers.long()\n",
    "    num_atoms = atomic_numbers.shape[0]\n",
    "\n",
    "    if self.regress_forces and not self.direct_forces:\n",
    "        pos.requires_grad_(True)\n",
    "\n",
    "    (\n",
    "        main_graph,\n",
    "        a2a_graph,\n",
    "        a2ee2a_graph,\n",
    "        qint_graph,\n",
    "        id_swap,\n",
    "        trip_idx_e2e,\n",
    "        trip_idx_a2e,\n",
    "        trip_idx_e2a,\n",
    "        quad_idx,\n",
    "    ) = self.get_graphs_and_indices(data)\n",
    "    _, idx_t = main_graph[\"edge_index\"]\n",
    "\n",
    "    (\n",
    "        basis_rad_raw,\n",
    "        basis_atom_update,\n",
    "        basis_output,\n",
    "        bases_qint,\n",
    "        bases_e2e,\n",
    "        bases_a2e,\n",
    "        bases_e2a,\n",
    "        basis_a2a_rad,\n",
    "    ) = self.get_bases(\n",
    "        main_graph=main_graph,\n",
    "        a2a_graph=a2a_graph,\n",
    "        a2ee2a_graph=a2ee2a_graph,\n",
    "        qint_graph=qint_graph,\n",
    "        trip_idx_e2e=trip_idx_e2e,\n",
    "        trip_idx_a2e=trip_idx_a2e,\n",
    "        trip_idx_e2a=trip_idx_e2a,\n",
    "        quad_idx=quad_idx,\n",
    "        num_atoms=num_atoms,\n",
    "    )\n",
    "\n",
    "    # Embedding block\n",
    "    h = self.atom_emb(atomic_numbers)\n",
    "    # (nAtoms, emb_size_atom)\n",
    "    m = self.edge_emb(h, basis_rad_raw, main_graph[\"edge_index\"])\n",
    "    # (nEdges, emb_size_edge)\n",
    "\n",
    "    x_E, x_F = self.out_blocks[0](h, m, basis_output, idx_t)\n",
    "    # (nAtoms, emb_size_atom), (nEdges, emb_size_edge)\n",
    "    xs_E, xs_F = [x_E], [x_F]\n",
    "\n",
    "    for i in range(self.num_blocks):\n",
    "        # Interaction block\n",
    "        h, m = self.int_blocks[i](\n",
    "            h=h,\n",
    "            m=m,\n",
    "            bases_qint=bases_qint,\n",
    "            bases_e2e=bases_e2e,\n",
    "            bases_a2e=bases_a2e,\n",
    "            bases_e2a=bases_e2a,\n",
    "            basis_a2a_rad=basis_a2a_rad,\n",
    "            basis_atom_update=basis_atom_update,\n",
    "            edge_index_main=main_graph[\"edge_index\"],\n",
    "            a2ee2a_graph=a2ee2a_graph,\n",
    "            a2a_graph=a2a_graph,\n",
    "            id_swap=id_swap,\n",
    "            trip_idx_e2e=trip_idx_e2e,\n",
    "            trip_idx_a2e=trip_idx_a2e,\n",
    "            trip_idx_e2a=trip_idx_e2a,\n",
    "            quad_idx=quad_idx,\n",
    "        )  # (nAtoms, emb_size_atom), (nEdges, emb_size_edge)\n",
    "\n",
    "        x_E, x_F = self.out_blocks[i + 1](h, m, basis_output, idx_t)\n",
    "        # (nAtoms, emb_size_atom), (nEdges, emb_size_edge)\n",
    "        xs_E.append(x_E)\n",
    "        xs_F.append(x_F)\n",
    "\n",
    "    # Global output block for final predictions\n",
    "    x_E = self.out_mlp_E(torch.cat(xs_E, dim=-1))\n",
    "    if self.direct_forces:\n",
    "        x_F = self.out_mlp_F(torch.cat(xs_F, dim=-1))\n",
    "    with torch.cuda.amp.autocast(False):\n",
    "        E_t = self.out_energy(x_E.float())\n",
    "        if self.direct_forces:\n",
    "            F_st = self.out_forces(x_F.float())\n",
    "\n",
    "    nMolecules = torch.max(batch) + 1\n",
    "    if self.extensive:\n",
    "        E_t = scatter_det(\n",
    "            E_t, batch, dim=0, dim_size=nMolecules, reduce=\"add\"\n",
    "        )  # (nMolecules, num_targets)\n",
    "    else:\n",
    "        E_t = scatter_det(\n",
    "            E_t, batch, dim=0, dim_size=nMolecules, reduce=\"mean\"\n",
    "        )  # (nMolecules, num_targets)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    if self.regress_forces:\n",
    "        if self.direct_forces:\n",
    "            if self.forces_coupled:  # enforce F_st = F_ts\n",
    "                nEdges = idx_t.shape[0]\n",
    "                id_undir = repeat_blocks(\n",
    "                    main_graph[\"num_neighbors\"] // 2,\n",
    "                    repeats=2,\n",
    "                    continuous_indexing=True,\n",
    "                )\n",
    "                F_st = scatter_det(\n",
    "                    F_st,\n",
    "                    id_undir,\n",
    "                    dim=0,\n",
    "                    dim_size=int(nEdges / 2),\n",
    "                    reduce=\"mean\",\n",
    "                )  # (nEdges/2, num_targets)\n",
    "                F_st = F_st[id_undir]  # (nEdges, num_targets)\n",
    "\n",
    "            # map forces in edge directions\n",
    "            F_st_vec = F_st[:, :, None] * main_graph[\"vector\"][:, None, :]\n",
    "            # (nEdges, num_targets, 3)\n",
    "            F_t = scatter_det(\n",
    "                F_st_vec,\n",
    "                idx_t,\n",
    "                dim=0,\n",
    "                dim_size=num_atoms,\n",
    "                reduce=\"add\",\n",
    "            )  # (nAtoms, num_targets, 3)\n",
    "        else:\n",
    "            F_t = self.force_scaler.calc_forces_and_update(E_t, pos)\n",
    "\n",
    "        out[\"energy\"] = E_t.squeeze(1)  # (num_molecules)\n",
    "        out[\"forces\"] = F_t.squeeze(1)  # (num_atoms, 3)\n",
    "    else:\n",
    "        out[\"energy\"] = E_t.squeeze(1)  # (num_molecules)\n",
    "\n",
    "    # This is the section I adapted from abishek's code\n",
    "    if hasattr(self, 'return_embedding') and self.return_embedding:\n",
    "        nMolecules = (torch.max(batch) + 1).item()\n",
    "\n",
    "        # This seems to be an earlier block\n",
    "        out[\"h sum\"] = scatter_det(\n",
    "            h, batch, dim=0, dim_size=nMolecules, reduce=\"add\"\n",
    "        )\n",
    "        \n",
    "        # These embedding are closer to energy output\n",
    "        out[\"x_E sum\"] = scatter_det(\n",
    "            x_E, batch, dim=0, dim_size=nMolecules, reduce=\"add\"\n",
    "        )\n",
    "\n",
    "        # This is an embedding related to forces.\n",
    "        # Something seems off on I couldn't do the same thing as above with scatter_det.\n",
    "        out[\"x_F sum\"] = torch.sum(x_F, axis=0)[None, :]\n",
    "        \n",
    "\n",
    "        # tuples with nMolecules tensors of size nAtoms x embedding_size.\n",
    "        out[\"x_E\"] = x_E.split(data.natoms.tolist(), dim=0)\n",
    "        \n",
    "        out[\"h\"] = h.split(data.natoms.tolist(), dim=0)\n",
    "\n",
    "        return out\n",
    "    else:\n",
    "        if self.regress_forces:\n",
    "            return out[\"energy\"], out[\"forces\"]\n",
    "        else:\n",
    "            return out[\"energy\"]\n",
    "\n",
    "GemNetOC.forward = newforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocpmodels.common.relaxation.ase_utils import OCPCalculator\n",
    "from ocpmodels.datasets import data_list_collater\n",
    "\n",
    "\n",
    "def embed(self, atoms):\n",
    "    self.trainer._unwrapped_model.return_embedding = True\n",
    "    data_object = self.a2g.convert(atoms)\n",
    "    batch_list = data_list_collater([data_object], otf_graph=True)\n",
    "\n",
    "    self.trainer.model.eval()\n",
    "    if self.trainer.ema:\n",
    "        self.trainer.ema.store()\n",
    "        self.trainer.ema.copy_to()\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=self.trainer.scaler is not None):\n",
    "        with torch.no_grad():\n",
    "            out = self.trainer.model([batch_list])\n",
    "\n",
    "    if (\n",
    "        self.trainer.normalizers is not None\n",
    "        and \"target\" in self.trainer.normalizers\n",
    "    ):\n",
    "        out[\"energy\"] = self.trainer.normalizers[\"target\"].denorm(\n",
    "            out[\"energy\"]\n",
    "        )\n",
    "        out[\"forces\"] = self.trainer.normalizers[\"grad_target\"].denorm(\n",
    "            out[\"forces\"]\n",
    "        )\n",
    "\n",
    "    if self.trainer.ema:\n",
    "        self.trainer.ema.restore()\n",
    "\n",
    "    return out\n",
    "\n",
    "OCPCalculator.embed = embed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
