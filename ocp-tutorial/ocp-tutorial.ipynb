{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73366e40",
   "metadata": {},
   "source": [
    "# Convenience functions for getting ocp paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3beb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ocpmodels as om\n",
    "from pathlib import Path\n",
    "\n",
    "def ocp_root():\n",
    "    \"\"\"Return the root directory of the installed ocp package.\"\"\"\n",
    "    return Path(om.__file__).parent.parent\n",
    "\n",
    "def ocp_main():\n",
    "    \"\"\"Return the path to ocp main.py\"\"\"\n",
    "    return ocp_root() / \"main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe4d6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import numba\n",
    "import numpy as np\n",
    "import ase\n",
    "import e3nn\n",
    "import pymatgen.core as pc\n",
    "import torch\n",
    "import torch.cuda as tc\n",
    "import torch_geometric as tg\n",
    "import platform\n",
    "import psutil\n",
    "\n",
    "\n",
    "def describe_ocp():\n",
    "    \"\"\"Print some system information that could be useful in debugging.\"\"\"\n",
    "    print(sys.executable, sys.version)  \n",
    "    print(f'ocp is installed at {ocp_root()}')\n",
    "    \n",
    "    commit_hash = (\n",
    "            subprocess.check_output(\n",
    "                    [\n",
    "                        \"git\",\n",
    "                        \"-C\",\n",
    "                        om.__path__[0],\n",
    "                        \"describe\",\n",
    "                        \"--always\",\n",
    "                    ]\n",
    "                )\n",
    "                .strip()\n",
    "                .decode(\"ascii\")\n",
    "            )\n",
    "    print(f'ocp repo is at git commit: {commit_hash}')\n",
    "    print(f'numba: {numba.__version__}')\n",
    "    print(f'numpy: {np.version.version}')\n",
    "    print(f'ase: {ase.__version__}')\n",
    "    print(f'e3nn: {e3nn.__version__}')\n",
    "    print(f'pymatgen: {pc.__version__}')\n",
    "    print(f'torch: {torch.version.__version__}')\n",
    "    print(f'torch.version.cuda: {torch.version.cuda}')\n",
    "    print(f'torch.cuda: is_available: {tc.is_available()}')\n",
    "    if tc.is_available():\n",
    "        print('  __CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "        print('  __Number CUDA Devices:', torch.cuda.device_count())\n",
    "        print('  __CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "        print('  __CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
    "    print(f'torch geometric: {tg.__version__}')    \n",
    "    print()\n",
    "    print(f'Platform: {platform.platform()}')\n",
    "    print(f'  Processor: {platform.processor()}')\n",
    "    print(f'  Virtual memory: {psutil.virtual_memory()}')\n",
    "    print(f'  Swap memory: {psutil.swap_memory()}')\n",
    "    print(f'  Disk usage: {psutil.disk_usage(\"/\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8939e5c",
   "metadata": {},
   "source": [
    "# Convenience function for getting checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46a551f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "checkpoints = {\n",
    "    # Open Catalyst 2020 (OC20)\n",
    "    'CGCNN 200k'\t:'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/cgcnn_200k.pt', \n",
    "    'CGCNN 2M'\t    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/cgcnn_2M.pt', \n",
    "    'CGCNN 20M'\t:'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/cgcnn_20M.pt', \n",
    "    'CGCNN All'\t:'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/cgcnn_all.pt', \n",
    "    'DimeNet 200k'\t:'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/dimenet_200k.pt',\n",
    "    'DimeNet 2M'\t:'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/dimenet_2M.pt',\n",
    "    'SchNet 200k'\t:'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/schnet_200k.pt', \n",
    "    'SchNet 2M'\t    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/schnet_2M.pt', \n",
    "    'SchNet 20M'\t:'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/schnet_20M.pt', \n",
    "    'SchNet All'\t:'https://dl.fbaipublicfiles.com/opencatalystproject/models/2020_11/s2ef/schnet_all_large.pt', \n",
    "    'DimeNet++ 200k'   :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2021_02/s2ef/dimenetpp_200k.pt', \n",
    "    'DimeNet++ 2M'     :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2021_02/s2ef/dimenetpp_2M.pt', \n",
    "    'DimeNet++ 20M'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2021_02/s2ef/dimenetpp_20M.pt', \n",
    "    'DimeNet++ All'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2021_02/s2ef/dimenetpp_all.pt', \n",
    "    'SpinConv 2M'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2021_12/s2ef/spinconv_force_centric_2M.pt', \n",
    "    'SpinConv All'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2021_08/s2ef/spinconv_force_centric_all.pt', \n",
    "    'GemNet-dT 2M'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2021_12/s2ef/gemnet_t_direct_h512_2M.pt', \n",
    "    'GemNet-dT All'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2021_08/s2ef/gemnet_t_direct_h512_all.pt', \n",
    "    'PaiNN All'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2022_05/s2ef/painn_h512_s2ef_all.pt', \n",
    "    'GemNet-OC 2M'     :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2022_07/s2ef/gemnet_oc_base_s2ef_2M.pt', \n",
    "    'GemNet-OC All'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2022_07/s2ef/gemnet_oc_base_s2ef_all.pt', \n",
    "    'GemNet-OC All+MD'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_03/s2ef/gemnet_oc_base_s2ef_all_md.pt', \n",
    "    'GemNet-OC-Large All+MD' :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2022_07/s2ef/gemnet_oc_large_s2ef_all_md.pt', \n",
    "    'SCN 2M'   :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_03/s2ef/scn_t1_b1_s2ef_2M.pt', \n",
    "    'SCN-t4-b2 2M'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_03/s2ef/scn_t4_b2_s2ef_2M.pt', \n",
    "    'SCN All+MD' :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_03/s2ef/scn_all_md_s2ef.pt', \n",
    "    'eSCN-L4-M2-Lay12 2M'     :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_03/s2ef/escn_l4_m2_lay12_2M_s2ef.pt', \n",
    "    'eSCN-L6-M2-Lay12 2M'    :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_03/s2ef/escn_l6_m2_lay12_2M_s2ef.pt', \n",
    "    'eSCN-L6-M2-Lay12 All+MD'     :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_03/s2ef/escn_l6_m2_lay12_all_md_s2ef.pt', \n",
    "    'eSCN-L6-M3-Lay20 All+MD'     :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_03/s2ef/escn_l6_m3_lay20_all_md_s2ef.pt', \n",
    "    'EquiformerV2 (83M) 2M'     :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_06/oc20/s2ef/eq2_83M_2M.pt', \n",
    "    'EquiformerV2 (31M) All+MD'     :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_06/oc20/s2ef/eq2_31M_ec4_allmd.pt', \n",
    "    'EquiformerV2 (153M) All+MD'     :'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_06/oc20/s2ef/eq2_153M_ec4_allmd.pt', \n",
    "    # Open Catalyst 2022 (OC22)\n",
    "    'GemNet-dT OC22'\t: 'https://dl.fbaipublicfiles.com/opencatalystproject/models/2022_09/oc22/s2ef/gndt_oc22_all_s2ef.pt',\n",
    "    'GemNet-OC OC22'\t: 'https://dl.fbaipublicfiles.com/opencatalystproject/models/2022_09/oc22/s2ef/gnoc_oc22_all_s2ef.pt',\n",
    "    'GemNet-OC OC20+OC22'\t: 'https://dl.fbaipublicfiles.com/opencatalystproject/models/2022_09/oc22/s2ef/gnoc_oc22_oc20_all_s2ef.pt',\n",
    "    'GemNet-OC trained with `enforce_max_neighbors_strictly=False` #467 OC20+OC22' : 'https://dl.fbaipublicfiles.com/opencatalystproject/models/2023_05/oc22/s2ef/gnoc_oc22_oc20_all_s2ef.pt',\n",
    "    'GemNet-OC OC20->OC22'\t: 'https://dl.fbaipublicfiles.com/opencatalystproject/models/2022_09/oc22/s2ef/gnoc_finetune_all_s2ef.pt'\n",
    "                }\n",
    "\n",
    "def list_checkpoints():\n",
    "    \"\"\"List checkpoints that are available to download.\"\"\"\n",
    "    print('See https://github.com/Open-Catalyst-Project/ocp/blob/main/MODELS.md for more details.')\n",
    "    for key in checkpoints:\n",
    "        print(key)\n",
    "    print('Copy one of these keys to get_checkpoint(key) to download it.')\n",
    "\n",
    "        \n",
    "def get_checkpoint(key):\n",
    "    \"\"\"Download a checkpoint.\n",
    "    \n",
    "    key: string in checkpoints.\n",
    "    \n",
    "    Returns name of checkpoint that was saved.\n",
    "    \"\"\"\n",
    "    url = checkpoints.get(key, None)\n",
    "    if url is None:\n",
    "        raise Exception('No url found for {key}')\n",
    "            \n",
    "    pt = Path(urllib.parse.urlparse(url).path).name\n",
    "        \n",
    "    if not os.path.exists(pt):\n",
    "        with open(pt, 'wb') as f:\n",
    "            print(f'Downloading {url}')\n",
    "            f.write(requests.get(url).content)\n",
    "    return pt      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dfd7da",
   "metadata": {},
   "source": [
    "# Train/test/val split for an ase db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd6260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from ase.db import connect\n",
    "\n",
    "def train_test_val_split(ase_db, ttv=(0.8, 0.1, .1), files=('train.db', 'test.db', 'val.db'), seed=42):\n",
    "    \"\"\"Split an ase db into train, test and validation dbs.\n",
    "    \n",
    "    ase_db: path to an ase db containing all the data.\n",
    "    ttv: a tuple containing the fraction of train, test and val data. This will be normalized.\n",
    "    files: a tuple of filenames to write the splits into. An exception is raised if these exist. \n",
    "           You should delete them first.\n",
    "    seed: an integer for the random number generator seed\n",
    "    \n",
    "    Returns the absolute path to files.\n",
    "    \"\"\"\n",
    "    \n",
    "    for db in files:\n",
    "        if os.path.exists(db):\n",
    "            raise Exception('{db} exists. Please delete it before proceeding.')\n",
    "            \n",
    "    src = connect(ase_db)\n",
    "    N = src.count()\n",
    "    \n",
    "    ttv = np.array(ttv)\n",
    "    ttv /= ttv.sum()\n",
    "    \n",
    "    train_end = int(N * ttv[0])\n",
    "    test_end = train_end + int(N * ttv[1])\n",
    "    \n",
    "    train = connect(files[0])\n",
    "    test = connect(files[1])\n",
    "    val = connect(files[2])\n",
    "    \n",
    "    ids = np.arange(1, N + 1)\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    rng.shuffle(ids)\n",
    "    \n",
    "    for _id in ids[0:train_end]:\n",
    "        row = src.get(id=int(_id))\n",
    "        train.write(row.toatoms())\n",
    "    \n",
    "    for _id in ids[train_end:test_end]:\n",
    "        row = src.get(id=int(_id))\n",
    "        test.write(row.toatoms())\n",
    "    \n",
    "    for _id in ids[test_end:]:\n",
    "        row = src.get(id=int(_id))\n",
    "        val.write(row.toatoms())\n",
    "        \n",
    "    return [Path(f).absolute() for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cee9e",
   "metadata": {},
   "source": [
    "# Generating a config from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bb3ac555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, dump\n",
    "from yaml import CLoader as Loader, CDumper as Dumper\n",
    "import torch\n",
    "import os\n",
    "from ocpmodels.common.relaxation.ase_utils import OCPCalculator\n",
    "from io import StringIO\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "def generate_yml_config(checkpoint_path, yml='run.yml', delete=(), update=()):\n",
    "    \"\"\"Generate a yml config file from an existing checkpoint file.\n",
    "    \n",
    "    checkpoint_path: string to path of an existing checkpoint\n",
    "    yml: name of file to write to.\n",
    "    pop: list of keys to remove from the config\n",
    "    update: dictionary of key:values to update\n",
    "    \n",
    "    Use a dot notation in update.\n",
    "    \n",
    "    Returns an absolute path to the generated yml file.\n",
    "    \"\"\"\n",
    "             \n",
    "    # You can't just read in the checkpoint with torch. The calculator does some things to it. \n",
    "    # Rather than recreate that here I just reuse the calculator machinery. I don't want to \n",
    "    # see the output though, so I capture it.\n",
    "\n",
    "    with contextlib.redirect_stdout(StringIO()) as _:\n",
    "        config = OCPCalculator(checkpoint_path=checkpoint_path).config\n",
    "                       \n",
    "    for key in delete:\n",
    "        if key in config and len(key.split('.')) == 1:\n",
    "            del config[key]\n",
    "        else:\n",
    "            keys = key.split('.')\n",
    "            if keys[0] in config:\n",
    "                d = config[keys[0]]\n",
    "            else:\n",
    "                continue\n",
    "            if isinstance(d, dict):\n",
    "                for k in keys[1:]:\n",
    "                    if isinstance(d[k], dict):\n",
    "                        d = d[k]\n",
    "                    else:\n",
    "                        if k in d:\n",
    "                            del d[k]\n",
    "                        \n",
    "    def nested_set(dic, keys, value):\n",
    "        for key in keys[:-1]:\n",
    "            dic = dic.setdefault(key, {})\n",
    "        dic[keys[-1]] = value\n",
    "               \n",
    "    for _key in update:\n",
    "        keys = _key.split('.')\n",
    "        nested_set(config, keys, update[_key])\n",
    "\n",
    "        \n",
    "    out = dump(config)\n",
    "    with open(yml, 'wb') as f:\n",
    "        f.write(out.encode('utf-8'))\n",
    "        \n",
    "    return Path(yml).absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477466e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
