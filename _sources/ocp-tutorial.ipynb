{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dfd7da",
   "metadata": {},
   "source": [
    "# Train/test/val split for an ase db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd6260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from ase.db import connect\n",
    "\n",
    "def train_test_val_split(ase_db, ttv=(0.8, 0.1, .1), files=('train.db', 'test.db', 'val.db'), seed=42):\n",
    "    \"\"\"Split an ase db into train, test and validation dbs.\n",
    "    \n",
    "    ase_db: path to an ase db containing all the data.\n",
    "    ttv: a tuple containing the fraction of train, test and val data. This will be normalized.\n",
    "    files: a tuple of filenames to write the splits into. An exception is raised if these exist. \n",
    "           You should delete them first.\n",
    "    seed: an integer for the random number generator seed\n",
    "    \n",
    "    Returns the absolute path to files.\n",
    "    \"\"\"\n",
    "    \n",
    "    for db in files:\n",
    "        if os.path.exists(db):\n",
    "            raise Exception('{db} exists. Please delete it before proceeding.')\n",
    "            \n",
    "    src = connect(ase_db)\n",
    "    N = src.count()\n",
    "    \n",
    "    ttv = np.array(ttv)\n",
    "    ttv /= ttv.sum()\n",
    "    \n",
    "    train_end = int(N * ttv[0])\n",
    "    test_end = train_end + int(N * ttv[1])\n",
    "    \n",
    "    train = connect(files[0])\n",
    "    test = connect(files[1])\n",
    "    val = connect(files[2])\n",
    "    \n",
    "    ids = np.arange(1, N + 1)\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    rng.shuffle(ids)\n",
    "    \n",
    "    for _id in ids[0:train_end]:\n",
    "        row = src.get(id=int(_id))\n",
    "        train.write(row.toatoms())\n",
    "    \n",
    "    for _id in ids[train_end:test_end]:\n",
    "        row = src.get(id=int(_id))\n",
    "        test.write(row.toatoms())\n",
    "    \n",
    "    for _id in ids[test_end:]:\n",
    "        row = src.get(id=int(_id))\n",
    "        val.write(row.toatoms())\n",
    "        \n",
    "    return [Path(f).absolute() for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cee9e",
   "metadata": {},
   "source": [
    "# Generating a config from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bb3ac555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, dump\n",
    "from yaml import CLoader as Loader, CDumper as Dumper\n",
    "import torch\n",
    "import os\n",
    "from ocpmodels.common.relaxation.ase_utils import OCPCalculator\n",
    "from io import StringIO\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "def generate_yml_config(checkpoint_path, yml='run.yml', delete=(), update=()):\n",
    "    \"\"\"Generate a yml config file from an existing checkpoint file.\n",
    "    \n",
    "    checkpoint_path: string to path of an existing checkpoint\n",
    "    yml: name of file to write to.\n",
    "    pop: list of keys to remove from the config\n",
    "    update: dictionary of key:values to update\n",
    "    \n",
    "    Use a dot notation in update.\n",
    "    \n",
    "    Returns an absolute path to the generated yml file.\n",
    "    \"\"\"\n",
    "             \n",
    "    # You can't just read in the checkpoint with torch. The calculator does some things to it. \n",
    "    # Rather than recreate that here I just reuse the calculator machinery. I don't want to \n",
    "    # see the output though, so I capture it.\n",
    "\n",
    "    with contextlib.redirect_stdout(StringIO()) as _:\n",
    "        config = OCPCalculator(checkpoint=os.path.expanduser(checkpoint)).config\n",
    "                       \n",
    "    for key in delete:\n",
    "        if key in config and len(key.split('.')) == 1:\n",
    "            del config[key]\n",
    "        else:\n",
    "            keys = key.split('.')\n",
    "            d = config[keys[0]]\n",
    "            if isinstance(d, dict):\n",
    "                for k in keys[1:]:\n",
    "                    if isinstance(d[k], dict):\n",
    "                        d = d[k]\n",
    "                    else:\n",
    "                        del d[k]\n",
    "                        \n",
    "    def nested_set(dic, keys, value):\n",
    "        for key in keys[:-1]:\n",
    "            dic = dic.setdefault(key, {})\n",
    "        dic[keys[-1]] = value\n",
    "               \n",
    "    for _key in update:\n",
    "        keys = _key.split('.')\n",
    "        nested_set(config, keys, update[_key])\n",
    "\n",
    "        \n",
    "    out = dump(config)\n",
    "    with open(yml, 'wb') as f:\n",
    "        f.write(out.encode('utf-8'))\n",
    "        \n",
    "    return Path(yml).absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477466e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
