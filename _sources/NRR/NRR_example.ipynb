{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3a772a-2ce9-46bd-a52e-b43210f30541",
   "metadata": {},
   "source": [
    "Using OCP to enumerate adsorbates on catalyst surfaces\n",
    "======================================================\n",
    "\n",
    "In the previous example, we constructed slab models of adsorbates on desired sites. Here we leverage code to automate this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d6813a-7c44-4585-8335-f211a11ff2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ocdata'...\n",
      "remote: Enumerating objects: 1741, done.\u001b[K\n",
      "remote: Counting objects: 100% (433/433), done.\u001b[K\n",
      "remote: Compressing objects: 100% (212/212), done.\u001b[K\n",
      "remote: Total 1741 (delta 260), reused 241 (delta 221), pack-reused 1308\u001b[K\n",
      "Receiving objects: 100% (1741/1741), 36.87 MiB | 13.40 MiB/s, done.\n",
      "Resolving deltas: 100% (1081/1081), done.\n",
      "Obtaining file:///home/jovyan/shared-scratch/jkitchin/tutorial/ocp-tutorial/NRR/ocdata\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: ocdata\n",
      "  Running setup.py develop for ocdata\n",
      "Successfully installed ocdata-0.2.0\n"
     ]
    }
   ],
   "source": [
    "! rm -fr ocdata\n",
    "! git clone https://github.com/Open-Catalyst-Project/Open-Catalyst-Dataset.git ocdata\n",
    "! cd ocdata && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4affd67-e7c1-4043-82ba-4f4e5d601360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: e3nn in /opt/conda/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from e3nn) (1.11.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from e3nn) (1.9.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.9/site-packages (from e3nn) (1.11.0)\n",
      "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /opt/conda/lib/python3.9/site-packages (from e3nn) (0.1.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from opt-einsum-fx>=0.1.4->e3nn) (21.3)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.9/site-packages (from opt-einsum-fx>=0.1.4->e3nn) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.8.0->e3nn) (4.4.0)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in /opt/conda/lib/python3.9/site-packages (from scipy->e3nn) (1.23.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->e3nn) (1.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->opt-einsum-fx>=0.1.4->e3nn) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "! pip install e3nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee85286-fcfe-452b-9017-7c0bce8168bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocpmodels.common.relaxation.ase_utils import OCPCalculator\n",
    "import ase.io\n",
    "from ase.optimize import BFGS\n",
    "import sys\n",
    "from scipy.stats import linregress\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ocdata.core import Adsorbate, AdsorbateSlabConfig, Bulk, Slab\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from ocdata.utils import DetectTrajAnomaly\n",
    "\n",
    "# Optional - see below\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ec38f-0637-4beb-bc77-18c9c3c1bf33",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Hello All! We are so glad you could join us to learn a bit more about using OCP models to accelerate chemical simulations. We will reproduce Fig 6b from the following paper: Zhou, Jing, et al. \"Enhanced Catalytic Activity of Bimetallic Ordered Catalysts for Nitrogen Reduction Reaction by Perturbation of Scaling Relations.\" ACS Catalysis 134 (2023): 2190-2201 (https://doi.org/10.1021/acscatal.2c05877).\n",
    "\n",
    "To do this, we will enumerate adsorbate-slab configurations and run ML relaxations on them to find the lowest energy configuration. We will assess parity between the model predicted values and those reported in the paper. Finally we will make the figure and assess separability of the NRR favored and HER favored domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774b7f5-e662-4959-9202-660d7bcaaacd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Enumerate the adsorbate-slab configurations to run relaxations on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b10e112-25be-43e2-8f7d-0cda02187746",
   "metadata": {},
   "source": [
    "Be sure to set the path in `ocdata/configs/paths.py` to point to the correct place or pass the paths as an argument. The database pickles can be found in `ocdata/databases/pkls`. We will show one explicitly here as an example and then run all of them in an automated fashion for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b654b99-f0f5-4bcb-9866-540ee3fc9f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jovyan/shared-scratch/jkitchin/tutorial/ocp-tutorial/NRR/ocdata/ocdata/databases/pkls/adsorbates.pkl')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ocdata\n",
    "from pathlib import Path\n",
    "db = Path(ocdata.__file__).parent / Path('databases/pkls/adsorbates.pkl')\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7132ce-9536-4b5f-a7f7-cf42a528b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_src_id = \"oqmd-343039\"\n",
    "adsorbate_smiles_nnh = \"*N*NH\"\n",
    "adsorbate_smiles_h = \"*H\"\n",
    "\n",
    "bulk = Bulk(bulk_src_id_from_db = bulk_src_id, bulk_db_path=\"NRR_example_bulks.pkl\")\n",
    "adsorbate_H = Adsorbate(adsorbate_smiles_from_db = adsorbate_smiles_h, adsorbate_db_path=db)\n",
    "adsorbate_NNH = Adsorbate(adsorbate_smiles_from_db = adsorbate_smiles_nnh, adsorbate_db_path=db)\n",
    "slab = Slab.from_bulk_get_specific_millers(bulk= bulk, specific_millers = (1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ab457f-5e19-4c5a-beaf-4430a8426601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform heuristic placements\n",
    "heuristic_adslabs = AdsorbateSlabConfig(slab[0], adsorbate_H, mode=\"heuristic\")\n",
    "\n",
    "# Perform random placements\n",
    "# (for AdsorbML we use `num_sites = 100` but we will use 4 for brevity here)\n",
    "random_adslabs = AdsorbateSlabConfig(slab[0], adsorbate_H, mode=\"random_site_heuristic_placement\", num_sites = 20)\n",
    "\n",
    "adslabs = [*heuristic_adslabs.atoms_list, *random_adslabs.atoms_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df1a82-46b1-4ab1-8c2c-e4983fe31747",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run ML relaxations:\n",
    "\n",
    "There are 2 options for how to do this.\n",
    " 1. Using `OCPCalculator` as the calculator within the ASE framework\n",
    " 2. By writing objects to lmdb and relaxing them using `main.py` in the ocp repo\n",
    " \n",
    "(1) is really only adequate for small stuff and it is what I will show here, but if you plan to run many relaxations, you should definitely use (2). More details about writing lmdbs has been provided [here](https://github.com/Open-Catalyst-Project/ocp/blob/main/tutorials/lmdb_dataset_creation.ipynb) - follow the IS2RS/IS2RE instructions. And more information about running relaxations once the lmdb has been written is [here](https://github.com/Open-Catalyst-Project/ocp/blob/main/TRAIN.md#initial-structure-to-relaxed-structure-is2rs).\n",
    "\n",
    "You need to provide the calculator with a path to a model checkpoint file. That can be downloaded [here](https://github.com/Open-Catalyst-Project/ocp/blob/main/MODELS.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c9853-29a4-4b44-b7d2-9e4f02f378ed",
   "metadata": {},
   "source": [
    "Running the model with BFGS prints at each relaxation step is a lot to print. So we will just run one to demonstrate what happens on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe297959-7de2-4e54-bca6-562b230de12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp: false\n",
      "cmd:\n",
      "  checkpoint_dir: /home/jovyan/shared-scratch/jkitchin/tutorial/ocp-tutorial/NRR/checkpoints/2023-07-16-16-12-48\n",
      "  commit: ab7833d\n",
      "  identifier: ''\n",
      "  logs_dir: /home/jovyan/shared-scratch/jkitchin/tutorial/ocp-tutorial/NRR/logs/tensorboard/2023-07-16-16-12-48\n",
      "  print_every: 100\n",
      "  results_dir: /home/jovyan/shared-scratch/jkitchin/tutorial/ocp-tutorial/NRR/results/2023-07-16-16-12-48\n",
      "  seed: null\n",
      "  timestamp_id: 2023-07-16-16-12-48\n",
      "dataset: null\n",
      "gpus: 0\n",
      "logger: tensorboard\n",
      "model: escn\n",
      "model_attributes:\n",
      "  basis_width_scalar: 2.0\n",
      "  cutoff: 12.0\n",
      "  distance_function: gaussian\n",
      "  hidden_channels: 384\n",
      "  lmax_list:\n",
      "  - 6\n",
      "  max_neighbors: 20\n",
      "  mmax_list:\n",
      "  - 3\n",
      "  num_layers: 20\n",
      "  num_sphere_samples: 128\n",
      "  otf_graph: true\n",
      "  regress_forces: true\n",
      "  sphere_channels: 160\n",
      "  use_pbc: true\n",
      "noddp: false\n",
      "optim:\n",
      "  batch_size: 3\n",
      "  clip_grad_norm: 20\n",
      "  ema_decay: 0.999\n",
      "  energy_coefficient: 4\n",
      "  eval_batch_size: 3\n",
      "  eval_every: 5000\n",
      "  force_coefficient: 100\n",
      "  loss_energy: mae\n",
      "  loss_force: l2mae\n",
      "  lr_gamma: 0.3\n",
      "  lr_initial: 0.0008\n",
      "  lr_milestones:\n",
      "  - 433166\n",
      "  - 541460\n",
      "  - 649750\n",
      "  max_epochs: 24\n",
      "  num_workers: 8\n",
      "  optimizer: AdamW\n",
      "  optimizer_params:\n",
      "    amsgrad: true\n",
      "  warmup_factor: 0.2\n",
      "  warmup_steps: 100\n",
      "slurm:\n",
      "  additional_parameters:\n",
      "    constraint: volta32gb\n",
      "  constraint: volta32gb\n",
      "  cpus_per_task: 9\n",
      "  folder: /checkpoint/zitnick/ocp_logs/4486283\n",
      "  gpus_per_node: 8\n",
      "  job_id: '4486283'\n",
      "  job_name: eSCN-L6-M3-Lay20-All-MD\n",
      "  mem: 480GB\n",
      "  nodes: 4\n",
      "  ntasks_per_node: 8\n",
      "  partition: ocp\n",
      "  time: 4320\n",
      "task:\n",
      "  dataset: trajectory_lmdb\n",
      "  description: Regressing to energies and forces for DFT trajectories from OCP\n",
      "  eval_on_free_atoms: true\n",
      "  grad_input: atomic forces\n",
      "  labels:\n",
      "  - potential energy\n",
      "  metric: mae\n",
      "  primary_metric: forces_mae\n",
      "  relax_opt:\n",
      "    alpha: 70.0\n",
      "    damping: 1.0\n",
      "    maxstep: 0.04\n",
      "    memory: 50\n",
      "    name: lbfgs\n",
      "    traj_dir: traj_id\n",
      "  relaxation_steps: 200\n",
      "  train_on_free_atoms: true\n",
      "  type: regression\n",
      "  write_pos: true\n",
      "trainer: forces\n",
      "\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data/oqmd-343039_H'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define the calculator\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# calc = OCPCalculator(checkpoint=checkpoint_path, cpu=False)   # if you have a GPU\u001b[39;00m\n\u001b[1;32m      9\u001b[0m calc \u001b[38;5;241m=\u001b[39m OCPCalculator(checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint_path, cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbulk_src_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_H\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m adslab \u001b[38;5;241m=\u001b[39m adslabs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m adslab\u001b[38;5;241m.\u001b[39mcalc \u001b[38;5;241m=\u001b[39m calc\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data/oqmd-343039_H'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "checkpoint_path = \"../escn_l6_m3_lay20_all_md_s2ef.pt\"\n",
    "os.makedirs(f\"data/{bulk_src_id}_{adsorbate_smiles_h}\", exist_ok=True)\n",
    "\n",
    "# Define the calculator\n",
    "# calc = OCPCalculator(checkpoint=checkpoint_path, cpu=False)   # if you have a GPU\n",
    "calc = OCPCalculator(checkpoint=checkpoint_path, cpu=True)\n",
    "\n",
    "os.mkdir(f\"data/{bulk_src_id}_H\")\n",
    "adslab = adslabs[0]\n",
    "adslab.calc = calc\n",
    "opt = BFGS(adslab, trajectory=f\"data/{bulk_src_id}_H/test.traj\")\n",
    "opt.run(fmax=0.05, steps=100)\n",
    "\n",
    "print(f'Elapsed time {time.time() -t0:1.1f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b2c6c-6b0c-419a-a41a-9fb900376486",
   "metadata": {},
   "source": [
    "## Run all the systems\n",
    "\n",
    "In principle you can run all the systems now. It takes about an hour though, and we leave that for a later exercise if you want. For now we will run the first two, and for later analysis we provide a results file of all the runs. Let's read in our reference file and take a look at what is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62eb21d-03a6-4dda-bfcf-37b96da8632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NRR_example_bulks.pkl\", \"rb\") as f:\n",
    "    bulks = pickle.load(f)\n",
    "    \n",
    "bulks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7137b84-3474-4ede-a153-c6849b4768fc",
   "metadata": {},
   "source": [
    "We have 19 bulk materials we will consider. Next we extract the `src-id` for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c8e5b-96b0-4c91-a7d9-46442b7d8fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_ids = [row['src_id'] for row in bulks]\n",
    "bulk_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1e27b-aea2-426f-bafb-9fc0423943df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bulk_src_id in bulk_ids[1:3]: \n",
    "    # Enumerate slabs and establish adsorbates\n",
    "    bulk = Bulk(bulk_src_id_from_db=bulk_src_id, bulk_db_path=\"NRR_example_bulks.pkl\")\n",
    "    slab = Slab.from_bulk_get_specific_millers(bulk= bulk, specific_millers=(1,1,1))\n",
    "\n",
    "    # Perform heuristic placements\n",
    "    heuristic_adslabs_H = AdsorbateSlabConfig(slab[0], adsorbate_H, mode=\"heuristic\")\n",
    "    heuristic_adslabs_NNH = AdsorbateSlabConfig(slab[0], adsorbate_NNH, mode=\"heuristic\")\n",
    "\n",
    "    #Run relaxations\n",
    "    os.makedirs(f\"data/{bulk_src_id}_H\", exist_ok=True)\n",
    "    os.makedirs(f\"data/{bulk_src_id}_NNH\", exist_ok=True)\n",
    "    \n",
    "\n",
    "    # Set up the calculator\n",
    "    for idx, adslab in enumerate(heuristic_adslabs_H.atoms_list):\n",
    "        adslab.calc = calc\n",
    "        opt = BFGS(adslab, trajectory=f\"data/{bulk_src_id}_H/{idx}.traj\", logfile=f\"data/{bulk_src_id}_H/{idx}.log\")\n",
    "        opt.run(fmax=0.05, steps=100)\n",
    "        \n",
    "    for idx, adslab in enumerate(heuristic_adslabs_NNH.atoms_list):\n",
    "        adslab.calc = calc\n",
    "        opt = BFGS(adslab, trajectory=f\"data/{bulk_src_id}_NNH/{idx}.traj\", logfile=f\"data/{bulk_src_id}_H/{idx}.log\")\n",
    "        opt.run(fmax=0.05, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd48f41-52d9-475f-9b11-ec48fc0dc529",
   "metadata": {},
   "source": [
    "# Parse the trajectories and post-process\n",
    "\n",
    "As a post-processing step we check to see if:\n",
    "1. the adsorbate desorbed\n",
    "2. the adsorbate disassociated\n",
    "3. the adsorbate intercalated\n",
    "4. the surface has changed\n",
    "\n",
    "We check these because the effect our referencing scheme and may result in erroneous energies. For (4), the relaxed surface should really be supplied as well. It will be necessary when correcting the SP / RX energies later. Since we don't have it here, we will ommit supplying it, and the detector will instead compare the initial and final slab from the adsorbate-slab relaxation trajectory. If a relaxed slab is provided, the detector will compare it and the slab after the adsorbate-slab relaxation. The latter is more correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d005711-c702-4362-b930-f5bb8a6bbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over trajs to extract results\n",
    "min_E = []\n",
    "for file_outer in glob(\"data/*\"):\n",
    "    ads = file_outer.split(\"_\")[1]\n",
    "    bulk = file_outer.split(\"/\")[1].split(\"_\")[0]\n",
    "    results = []\n",
    "    for file in glob(f\"{file_outer}/*.traj\"):\n",
    "        rx_id = file.split(\"/\")[-1].split(\".\")[0]\n",
    "        traj = ase.io.read(file, \":\")\n",
    "\n",
    "        # Check to see if the trajectory is anomolous\n",
    "        detector = DetectTrajAnomaly(traj[0], traj[-1], traj[0].get_tags())\n",
    "        anom = (\n",
    "            detector.is_adsorbate_dissociated()\n",
    "            or detector.is_adsorbate_desorbed()\n",
    "            or detector.has_surface_changed()\n",
    "            or detector.is_adsorbate_intercalated()\n",
    "        )\n",
    "        rx_energy = traj[-1].get_potential_energy()\n",
    "        results.append({\"relaxation_idx\": rx_id, \"relaxed_atoms\": traj[-1],\n",
    "                        \"relaxed_energy_ml\": rx_energy, \"anomolous\": anom})\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    df = df[~df.anomolous].copy().reset_index()\n",
    "    min_e = min(df.relaxed_energy_ml.tolist())\n",
    "    min_E.append({\"adsorbate\":ads, \"bulk_id\":bulk, \"min_E_ml\": min_e})\n",
    "\n",
    "df = pd.DataFrame(min_E)\n",
    "df_h = df[df.adsorbate == \"H\"]\n",
    "df_nnh = df[df.adsorbate == \"NNH\"]\n",
    "df_flat = df_h.merge(df_nnh, on = \"bulk_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e85ee6-eef7-48c1-97fe-2f4762715a65",
   "metadata": {},
   "source": [
    "# Make parity plots for values obtained by ML v. reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5caf7-8bcc-474f-9cda-227954a45e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add literature data to the dataframe\n",
    "with open(\"literature_data.pkl\", \"rb\") as f:\n",
    "    literature_data = pickle.load(f)\n",
    "df_all = df_flat.merge(pd.DataFrame(literature_data), on = \"bulk_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd56dd9-1130-4f0e-8bd8-7b8f15f815f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "f.set_figheight(15)\n",
    "x = df_all.min_E_ml_x.tolist()\n",
    "y = df_all.E_lit_H.tolist()\n",
    "ax1.set_title(\"*H parity\")\n",
    "ax1.plot([-3.5, 2], [-3.5, 2], \"k-\", linewidth=3)\n",
    "slope, intercept, r, p, se = linregress(x, y)\n",
    "ax1.plot(\n",
    "    [-3.5, 2],\n",
    "    [\n",
    "        -3.5 * slope + intercept,\n",
    "        2 * slope + intercept,\n",
    "    ],\n",
    "    \"k--\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "ax1.legend(\n",
    "    [\n",
    "        \"y = x\",\n",
    "        f\"y = {slope:1.2f} x + {intercept:1.2f}, R-sq = {r**2:1.2f}\",\n",
    "    ],\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "ax1.scatter(x, y)\n",
    "ax1.axis(\"square\")\n",
    "ax1.set_xlim([-3.5, 2])\n",
    "ax1.set_ylim([-3.5, 2])\n",
    "ax1.set_xlabel(\"dE predicted OCP [eV]\")\n",
    "ax1.set_ylabel(\"dE NRR paper [eV]\");\n",
    "\n",
    "\n",
    "x = df_all.min_E_ml_y.tolist()\n",
    "y = df_all.E_lit_NNH.tolist()\n",
    "ax2.set_title(\"*N*NH parity\")\n",
    "ax2.plot([-3.5, 2], [-3.5, 2], \"k-\", linewidth=3)\n",
    "slope, intercept, r, p, se = linregress(x, y)\n",
    "ax2.plot(\n",
    "    [-3.5, 2],\n",
    "    [\n",
    "        -3.5 * slope + intercept,\n",
    "        2 * slope + intercept,\n",
    "    ],\n",
    "    \"k--\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "ax2.legend(\n",
    "    [\n",
    "        \"y = x\",\n",
    "        f\"y = {slope:1.2f} x + {intercept:1.2f}, R-sq = {r**2:1.2f}\",\n",
    "    ],\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "ax2.scatter(x, y)\n",
    "ax2.axis(\"square\")\n",
    "ax2.set_xlim([-3.5, 2])\n",
    "ax2.set_ylim([-3.5, 2])\n",
    "ax2.set_xlabel(\"dE predicted OCP [eV]\")\n",
    "ax2.set_ylabel(\"dE NRR paper [eV]\");\n",
    "f.set_figwidth(15)\n",
    "f.set_figheight(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af70be-b488-405c-9628-ab19791a0a1d",
   "metadata": {},
   "source": [
    "# Make figure 6b and compare to literature results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bbfb2c-81ff-4889-8650-0a59ac347b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "x = df_all[df_all.reaction == \"HER\"].min_E_ml_y.tolist()\n",
    "y = df_all[df_all.reaction == \"HER\"].min_E_ml_x.tolist()\n",
    "comp = df_all[df_all.reaction == \"HER\"].composition.tolist()\n",
    "\n",
    "ax1.scatter(x, y,c= \"r\", label = \"HER\")\n",
    "for i, txt in enumerate(comp):\n",
    "    ax1.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "x = df_all[df_all.reaction == \"NRR\"].min_E_ml_y.tolist()\n",
    "y = df_all[df_all.reaction == \"NRR\"].min_E_ml_x.tolist()\n",
    "comp = df_all[df_all.reaction == \"NRR\"].composition.tolist()\n",
    "ax1.scatter(x, y,c= \"b\", label = \"NRR\")\n",
    "for i, txt in enumerate(comp):\n",
    "    ax1.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(\"dE *N*NH predicted OCP [eV]\")\n",
    "ax1.set_ylabel(\"dE *H predicted OCP [eV]\")\n",
    "\n",
    "\n",
    "x = df_all[df_all.reaction == \"HER\"].E_lit_NNH.tolist()\n",
    "y = df_all[df_all.reaction == \"HER\"].E_lit_H.tolist()\n",
    "comp = df_all[df_all.reaction == \"HER\"].composition.tolist()\n",
    "\n",
    "ax2.scatter(x, y,c= \"r\", label = \"HER\")\n",
    "for i, txt in enumerate(comp):\n",
    "    ax2.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "x = df_all[df_all.reaction == \"NRR\"].E_lit_NNH.tolist()\n",
    "y = df_all[df_all.reaction == \"NRR\"].E_lit_H.tolist()\n",
    "comp = df_all[df_all.reaction == \"NRR\"].composition.tolist()\n",
    "ax2.scatter(x, y,c= \"b\", label = \"NRR\")\n",
    "for i, txt in enumerate(comp):\n",
    "    ax2.annotate(txt, (x[i], y[i]))\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(\"dE *N*NH literature [eV]\")\n",
    "ax2.set_ylabel(\"dE *H literature [eV]\")\n",
    "f.set_figwidth(15)\n",
    "f.set_figheight(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc9509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
